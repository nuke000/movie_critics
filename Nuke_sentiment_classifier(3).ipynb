{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236bb773-6e1d-4a38-ae01-c18542216287",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, load the data for sentiment task and prepare the data.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('https://github.com/mbburova/MDS/raw/main/sentiment.csv', index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70185214-a1b9-4ee5-bd2a-c4e24e8ded6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "tag_regexp = re.compile(\"<[^>]*>\")\n",
    "regex = re.compile(\"[A-Za-z-]+\")\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    text = re.sub(tag_regexp, '', text)\n",
    "    text = re.sub('\\s+', ' ',text)\n",
    "    text = re.sub(r'\\\\','', text)\n",
    "    text = text.lower().strip()\n",
    "    try:\n",
    "        return \" \".join(regex.findall(text))\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "data['cleaned_review'] = data['review'].apply(words_only)\n",
    "data['tokenized'] = data['cleaned_review'].apply(lambda x: x.split())\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ccaeb2-49ef-4239-813f-07db5c82fc1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data on train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.tokenized,data.sentiment, test_size=0.2, random_state = 5)\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b2d9fb-b5d5-46c4-8764-bf9adc4a4bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b91437-d6d2-46ba-95ae-30161cb8c2ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "STOPWORDS = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2261efc8-5704-455a-8dd3-8edc95bd8b35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a03148-1370-4c35-93a3-bb3e5be522e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DICT_SIZE = 500\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(X_train.sum())\n",
    "for word in list(counter):\n",
    "    if word in STOPWORDS:\n",
    "        del counter[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5342ee19-567f-4ccc-9360-5f30010f1ace",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "words_counts =  counter\n",
    "WORDS_TO_INDEX = [word[0] for word in counter.most_common(DICT_SIZE)]\n",
    "\n",
    "def BoW(words, words_to_index, dict_size):\n",
    "    \"\"\"\n",
    "        words: a list of words\n",
    "        dict_size: size of the dictionary\n",
    "        return a vector which is a bag-of-words representation of 'text'\n",
    "    \"\"\"\n",
    "    result_vector = np.array([0 for i in range(dict_size)])\n",
    "    wti = np.array(words_to_index)\n",
    "    \n",
    "    for word in words:\n",
    "        ind = np.where(wti == word)\n",
    "        if len (ind) == 1 :\n",
    "            result_vector[ind[0]] +=1\n",
    "    return result_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c334855d-420e-43a4-98a7-0c1307c86c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import sparse as sp_sparse\n",
    "X_train_bow = sp_sparse.vstack([sp_sparse.csr_matrix(BoW(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\n",
    "X_test_bow = sp_sparse.vstack([sp_sparse.csr_matrix(BoW(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test])\n",
    "print('X_train shape ', X_train_bow.shape)\n",
    "print('X_test shape ', X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc04a87-5e91-4606-918b-a0cf6e93bfba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators = 300, random_state=5, max_depth = 5)\n",
    "\n",
    "rfc = rfc.fit(X_train_bow, y_train)\n",
    "\n",
    "pred = rfc.predict(X_test_bow)\n",
    "\n",
    "accuracy = accuracy_score(y_test, pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e500022b-f58c-431b-86d0-ad07a1c3eb4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9585b77-6523-4eb5-b2c5-6e3097d71cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53df7e21-830b-4b0a-a2cd-20a7e789a4d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_sample = [['i','must','say','it','s', 'perfect'],['i','feel','horrible']]\n",
    "X_sample_bow = sp_sparse.vstack([sp_sparse.csr_matrix(BoW(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1793f0dc-33b6-4415-8f56-c64be5c6cfc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rfc.predict(X_sample_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "382eed06-faac-42ff-b68f-9329babc46cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SentimentClassifier.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SentimentClassifier.py\n",
    "from scipy import sparse as sp_sparse\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def words_only(text):\n",
    "    tag_regexp = re.compile(\"<[^>]*>\")\n",
    "    regex = re.compile(\"[A-Za-z-]+\")\n",
    "    text = re.sub(tag_regexp, '', text)\n",
    "    text = re.sub('\\s+', ' ',text)\n",
    "    text = re.sub(r'\\\\','', text)\n",
    "    text = text.lower().strip()\n",
    "    try:\n",
    "        return \" \".join(regex.findall(text))\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def BoW(words, words_to_index, dict_size):\n",
    "    \"\"\"\n",
    "        words: a list of words\n",
    "        dict_size: size of the dictionary\n",
    "        return a vector which is a bag-of-words representation of 'text'\n",
    "    \"\"\"\n",
    "    result_vector = np.array([0 for i in range(dict_size)])\n",
    "    wti = np.array(words_to_index)\n",
    "    \n",
    "    for word in words:\n",
    "        ind = np.where(wti == word)\n",
    "        if len (ind) == 1 :\n",
    "            result_vector[ind[0]] +=1\n",
    "    return result_vector\n",
    "\n",
    "class SentimentClassifier:\n",
    "    def __init__(self):\n",
    "        self._data = []\n",
    "        self._datapath = 'https://github.com/mbburova/MDS/raw/main/sentiment.csv'\n",
    "        self._model = RandomForestClassifier(n_estimators = 300, random_state=5, max_depth = 5)\n",
    "        self._X_train = []\n",
    "        self._X_test = []\n",
    "        self._y_train = []\n",
    "        self._y_test = []\n",
    "        self._DICT_SIZE = 500\n",
    "        self._WORDS_TO_INDEX = []\n",
    "        self._accuracy = 0\n",
    "    \n",
    "    def fit_model(self, datapath = 'https://github.com/mbburova/MDS/raw/main/sentiment.csv') :\n",
    "        self._data = pd.read_csv(datapath, index_col=0)\n",
    "        self._datapath = datapath\n",
    "        tag_regexp = re.compile(\"<[^>]*>\")\n",
    "        regex = re.compile(\"[A-Za-z-]+\")\n",
    "        self._data['cleaned_review'] = self._data['review'].apply(words_only)\n",
    "        self._data['tokenized'] = self._data['cleaned_review'].apply(lambda x: x.split())\n",
    "        self._X_train, self._X_test, self._y_train, self._y_test = train_test_split(self._data['tokenized'],self._data['sentiment'], test_size=0.2, random_state = 5)\n",
    "        \n",
    "        nltk.download('stopwords')\n",
    "        STOPWORDS = set(stopwords.words('english'))\n",
    "        \n",
    "        counter = Counter(self._X_train.sum())\n",
    "        for word in list(counter):\n",
    "            if word in STOPWORDS:\n",
    "                del counter[word]\n",
    "        \n",
    "        words_counts =  counter\n",
    "        self._WORDS_TO_INDEX = [word[0] for word in counter.most_common(self._DICT_SIZE)]\n",
    "        \n",
    "        X_train_bow = sp_sparse.vstack([sp_sparse.csr_matrix(BoW(text, self._WORDS_TO_INDEX, self._DICT_SIZE)) for text in self._X_train])\n",
    "        X_test_bow = sp_sparse.vstack([sp_sparse.csr_matrix(BoW(text, self._WORDS_TO_INDEX, self._DICT_SIZE)) for text in self._X_test])\n",
    "        \n",
    "        self._model = RandomForestClassifier(n_estimators = 300, random_state=5, max_depth = 5)\n",
    "        self._model = self._model.fit(X_train_bow, self._y_train)\n",
    "        pred = self._model.predict(X_test_bow)\n",
    "        self._accuracy = accuracy_score(self._y_test, pred)\n",
    "        \n",
    "        return self._model\n",
    "    \n",
    "    def predict(self, sample):\n",
    "        X_sample = [words_only(sample).split()]\n",
    "        #X_sample = [['i','must','say','it','s', 'perfect'],['i','feel','horrible']]\n",
    "        X_sample_bow = sp_sparse.vstack([sp_sparse.csr_matrix(BoW(text, self._WORDS_TO_INDEX, self._DICT_SIZE)) for text in X_sample])\n",
    "        pred = self._model.predict(X_sample_bow)\n",
    "        return pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0424ea-d29a-4e3d-8f37-1f14ad4081ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from SentimentClassifier import SentimentClassifier\n",
    "\n",
    "sc = SentimentClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f6a31b-31f2-410d-b9a3-c772750865a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a5ca2-bb5c-4b43-a930-5e32e638e0fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc._accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f2115e-59b5-4f6e-8e66-69b4cafbcaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc._accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c42a118-90c8-4515-811f-30a7fbe71350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.predict(\"I'm disapointed by the horrible plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aedec9-474b-4d86-9976-5c8b74a289d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.predict(\"I liked the scene with a car crash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c061c2b1-0da4-4ef9-9cab-6df80adc545b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.predict(\"You'll have to have your wits about you and your brain fully switched on watching Oppenheimer as it could easily get away from a nonattentive viewer. This is intelligent filmmaking which shows it's audience great respect. It fires dialogue packed with information at a relentless pace and jumps to very different times in Oppenheimer's life continuously through it's 3 hour runtime. There are visual clues to guide the viewer through these times but again you'll have to get to grips with these quite quickly. This relentlessness helps to express the urgency with which the US attacked it's chase for the atomic bomb before Germany could do the same. An absolute career best performance from (the consistenly brilliant) Cillian Murphy anchors the film. This is a nailed on Oscar performance. In fact the whole cast are fantastic (apart maybe for the sometimes overwrought Emily Blunt performance). RDJ is also particularly brilliant in a return to proper acting after his decade or so of calling it in. The screenplay is dense and layered (I'd say it was a thick as a Bible), cinematography is quite stark and spare for the most part but imbued with rich, lucious colour in moments (especially scenes with Florence Pugh), the score is beautiful at times but mostly anxious and oppressive, adding to the relentless pacing. The 3 hour runtime flies by. All in all I found it an intense, taxing but highly rewarding watch. This is film making at it finest. A really great watch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2c6e9f-ab49-4951-87f1-db5209ee85ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sc.predict(\"A word of advice. Don't waste your money and time going to see the Barbie movie. Went with the whole family (including my 12 year old daughter and wife) and they asked to leave before it was over, it was that bad. I think it's been 30 years since I walked out of a movie theater because the movie was so, so bad. I don't know how it's been so successful. Marketing at its best (or worst). It's boring, disjointed, and sexist (to both men and women). It's an unwatchable mess, and by the last 20 minutes we cared so little about the characters, that we just left. It's horrendous. Save two hours of your life and don't bother watching.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f79b1031-3895-4462-af6d-efa0f2216387",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile server.py\n",
    "from flask import Flask, request\n",
    "import json\n",
    "import pickle\n",
    "import re\n",
    "from SentimentClassifier import SentimentClassifier\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "model = SentimentClassifier()\n",
    "model.fit_model()\n",
    "\n",
    "@app.route('/')\n",
    "def help_message():\n",
    "    message = \"Use /sentiment path to classify the sentiment of the movie review.\\n\" \n",
    "    message += \"Post json format {'review': 'text of review for classification'}.\\n\" \n",
    "    message += (\" Current model accuracy: \" + str(model._accuracy))\n",
    "    return message\n",
    "\n",
    "@app.route('/sentiment', methods=[\"GET\", \"POST\"])\n",
    "def sentim_classifier():\n",
    "    if request.method == 'POST':\n",
    "        rq = request.get_json(force=True)\n",
    "        review = rq['review']\n",
    "        result = rewiew\n",
    "        #result = model.predict(review)\n",
    "        response = {\n",
    "            \"result\": result\n",
    "        }\n",
    "        return json.dumps(response)\n",
    "    else:\n",
    "        return \"You should use only POST query\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(\"0.0.0.0\", 8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5511d07c-4a81-4e2f-a22e-037c8e507a50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "! launch-server.sh server.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8406e5d9-84df-4f4e-ac02-53488af05a55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use /sentiment path to classify the sentiment of the movie review.\n"
     ]
    }
   ],
   "source": [
    "! curl http://localhost:8000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7b149ed-0ceb-4822-a9eb-9822575729e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You should use only POST query"
     ]
    }
   ],
   "source": [
    "! curl http://localhost:8000/sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15920afa-ea6a-45ff-a7b3-eb8cc96c4c1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'review': \"You'll have to have your wits about you and your brain fully switched on watching Oppenheimer as it could easily get away from a nonattentive viewer. This is intelligent filmmaking which shows it's audience great respect. It fires dialogue packed with information at a relentless pace and jumps to very different times in Oppenheimer's life continuously through it's 3 hour runtime. There are visual clues to guide the viewer through these times but again you'll have to get to grips with these quite quickly. This relentlessness helps to express the urgency with which the US attacked it's chase for the atomic bomb before Germany could do the same. An absolute career best performance from (the consistenly brilliant) Cillian Murphy anchors the film. This is a nailed on Oscar performance. In fact the whole cast are fantastic (apart maybe for the sometimes overwrought Emily Blunt performance). RDJ is also particularly brilliant in a return to proper acting after his decade or so of calling it in. The screenplay is dense and layered (I'd say it was a thick as a Bible), cinematography is quite stark and spare for the most part but imbued with rich, lucious colour in moments (especially scenes with Florence Pugh), the score is beautiful at times but mostly anxious and oppressive, adding to the relentless pacing. The 3 hour runtime flies by. All in all I found it an intense, taxing but highly rewarding watch. This is film making at it finest. A really great watch.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfe1bf43-70be-46c3-8f5c-ea7356f1a0eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2747ad51-6d36-4775-aa42-edcfe828e857",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = requests.post(\"http://localhost:8000/sentiment\", json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d3101cf-3cd9-40d3-8d18-d9466a910c35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f53aea3-eeea-455b-83af-86ca4b840de4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': {'review': \"You'll have to have your wits about you and your brain fully switched on watching Oppenheimer as it could easily get away from a nonattentive viewer. This is intelligent filmmaking which shows it's audience great respect. It fires dialogue packed with information at a relentless pace and jumps to very different times in Oppenheimer's life continuously through it's 3 hour runtime. There are visual clues to guide the viewer through these times but again you'll have to get to grips with these quite quickly. This relentlessness helps to express the urgency with which the US attacked it's chase for the atomic bomb before Germany could do the same. An absolute career best performance from (the consistenly brilliant) Cillian Murphy anchors the film. This is a nailed on Oscar performance. In fact the whole cast are fantastic (apart maybe for the sometimes overwrought Emily Blunt performance). RDJ is also particularly brilliant in a return to proper acting after his decade or so of calling it in. The screenplay is dense and layered (I'd say it was a thick as a Bible), cinematography is quite stark and spare for the most part but imbued with rich, lucious colour in moments (especially scenes with Florence Pugh), the score is beautiful at times but mostly anxious and oppressive, adding to the relentless pacing. The 3 hour runtime flies by. All in all I found it an intense, taxing but highly rewarding watch. This is film making at it finest. A really great watch.\"}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f44e08-038c-43ed-9965-b814362b2161",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
